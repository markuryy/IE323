# Measuring the Obvious: A Critical Framework for Kiosk Accessibility Evaluation

## Introduction: Why We Need to Rethink Accessibility Evaluation

Let's be direct: this is an undergraduate course project examining accessibility issues in public kiosks. We have zero budget, limited time, and a moral obligation not to waste people's time proving things we can see with our eyes. But more importantly, we have an opportunity to challenge how accessibility evaluation is conducted.

The current standard approach would have us either:
1. Recruit able-bodied college students to generate data proving kiosks work great for able-bodied college students
2. Somehow convince wheelchair users to demonstrate obviously problematic design choices for the price of a Big Mac
3. Pretend these limitations don't exist and write methodology that ignores reality

Instead, we're proposing something more honest and potentially more valuable: using empirical measurements and critical analysis to evaluate fundamental design issues without requiring anyone to perform their marginalization for our research.

## The Problem with Current Standards

We're attempting to evaluate modern touchscreen kiosks using accessibility standards written in 2010 - before most of these systems even existed. The ADA standards we're measuring against:
- Predate widespread touchscreen kiosk deployment
- Don't specifically address interactive digital interfaces
- Focus on physical accessibility without considering multimodal interaction
- Fail to address conflicts between different accessibility needs

This temporal disconnect means we're trying to evaluate 2024 technology against 2010 standards, but even this misses the larger point. Companies like McDonald's, with their armies of food scientists, business analysts, and tech experts, aren't actually constrained by 14-year-old ADA requirements. They choose to treat them as a ceiling rather than a floor. Simple, low-cost solutions like adjustable VESA mounts, matte screen protectors, or just selecting more appropriate displays exist - but aesthetic considerations and brand image consistently trump actual accessibility. When a corporation spending thousands per kiosk won't invest in basic adjustable mounting hardware that's been standard in ergonomic design for decades, we don't need user testing to identify the priority gap. 

## The Problem Being Perpetuated through Academia

The current standard approach is, in this case, an affront to data collection that will inevitably be predictable: confirming that the same pool of able-bodied people can in fact use an interface designed specifically for them. This pattern is particularly troubling in accessibility research, where student researchers are expected to validate designs by testing them on their peers - creating an endless cycle of able-bodied college students confirming that systems work great for able-bodied college students. This methodology doesn't just fail to identify real accessibility issues; it actively generates data that can be used to justify excluding marginalized users.

## Why Measurement-Based Evaluation Makes More Sense

Our methodology focuses on physical measurements and specification analysis for several critical reasons:

1. Observable Facts Don't Need Proof
- We don't need user testing to prove that glossy vertical screens under fluorescent lights create glare
- We don't need wheelchair users to demonstrate that eye-level screens are difficult to use from a seated position
- We don't need studies to show that making interface elements smaller (the current "wheelchair mode" solution) conflicts with visual accessibility needs

2. Measurable Characteristics Tell the Story
- Screen height and viewing angles can be evaluated against established ergonomic principles
- Glare and reflectivity can be quantified under actual usage conditions
- Space requirements and clearances can be measured against known standards
- Interface element sizes can be documented in different modes

3. Real-World Conditions Matter
- Environmental factors like lighting and noise affect usability
- Traffic patterns and space constraints impact accessibility
- Time pressure and environmental stressors affect interaction
- These can all be documented without requiring user demonstration

## The Infrastructure Reality

Modern kiosk implementations reveal priorities misaligned with accessibility:

1. Hardware Overkill
- Full Windows installations for what are essentially web browsers
- Processing power far exceeding interface requirements
- Multiple $3000+ machines per location
- Licensing costs that could pay worker wages

2. Physical Design Choices
- Vertical mounting that maximizes visual impact over usability
- High-gloss screens that prioritize aesthetics over visibility
- Complex interfaces that could be simplified
- Space requirements that often exceed necessary footprint

## The Compliance Paradox

Our measurements reveal how technical compliance can actually create practical barriers:

1. The "Big Screen" Problem
- Large displays marketed as accessibility features
- Mounted at heights that create access issues
- Vertical orientation that maximizes glare
- "Solutions" that invalidate the supposed benefits

2. Conflicting Accommodations
- Wheelchair mode that contradicts visual accessibility features
- Height adjustments that affect interface visibility
- Standard compliance that doesn't guarantee usability

## Measurement Protocol

Our empirical evaluation framework focuses on documenting observable characteristics that impact accessibility:

1. Physical Measurements
- Screen height from ground level
- Viewing angles from both standing and seated positions
- Reach requirements for all interactive elements
- Clear floor space and approach paths
- Surface reflectivity under typical lighting conditions
- Force required for physical interactions (card insertion, receipt retrieval)

2. Interface Documentation
- Element sizes in standard and "accessible" modes
- Workflow steps for common tasks
- Mode-switching mechanisms and their accessibility
- Response time and feedback mechanisms
- Error recovery options

3. Environmental Analysis
- Lighting conditions and glare patterns throughout the day
- Ambient noise levels affecting audio feedback
- Traffic patterns and space constraints
- Queue formation and social pressure factors

## The Cost of "Automation"

Let's talk about what these kiosks actually represent in terms of infrastructure:

1. Hardware Costs
- Multiple $3000+ machines per location
- Full OEM Windows licenses (activated!) for glorified web browsers
- Processing power exceeding interface needs
- Regular maintenance and updates required

2. Hidden Costs
- Power consumption for constant operation
- Technical support infrastructure
- Regular software updates and maintenance
- Physical space requirements
- Staff still needed to assist when systems fail

3. Opportunity Costs
- Windows licensing fees that could pay worker wages
- Space that could be used for additional service points
- Resources that could be invested in actual accessibility improvements

## The Ethics of Accessibility Testing

Traditional user testing in this context raises several ethical concerns:

1. Participant Recruitment
- Limited budget means biased sampling
- Ethical issues with asking marginalized groups to demonstrate known problems
- No compensation for participants' time and effort
- Potential physical and emotional stress

2. Data Quality
- Sample bias from available participants
- Artificial testing environment
- Time limitations of course project
- Limited scope of single-session testing

3. Systemic Issues
- Perpetuating the burden of proof on marginalized groups
- Validating designs that fail fundamental accessibility requirements
- Generating data that could be used to justify inadequate solutions

## Standards in a Rapidly Evolving Context

The 2010 ADA Standards for Accessible Design couldn't address:
- Widespread touchscreen deployment
- Complex multi-modal interactions
- Digital interface requirements
- Conflicts between different accessibility needs

This creates several challenges:
1. No specific guidance for digital interface accessibility
2. Unclear application of physical standards to digital elements
3. No consideration of multi-modal interaction requirements
4. Limited addressing of conflicting accommodation needs

## Looking Forward

This framework suggests several critical directions:

1. Standards Evolution
- Need for updated guidelines addressing digital interfaces
- Better integration of multiple accessibility needs
- More comprehensive evaluation criteria
- Recognition of real-world usage patterns

2. Evaluation Methods
- Focus on empirical measurement where possible
- Reduce redundant demonstration by visual and logical recognition of self-evident barriers
- Establishing evaluation protocols that don't require marginalized users to validate obvious design flaws
- Better integration of different accessibility requirements
- More efficient use of limited resources

3. Design Philosophy
- Prioritizing universal design principles
- Addressing conflicts between different needs
- Considering real-world usage patterns
- Balancing automation with accessibility

## Conclusion

Our methodology isn't just a practical choice given project constraints: it's an argument for fundamentally rethinking how we evaluate accessibility. By focusing on measurable characteristics and obvious design issues, we can identify and document accessibility barriers more efficiently and ethically. Sometimes the most rigorous approach is also the most obvious: measure what we can measure, and stop asking people to prove what we can see with our own eyes.

The fact that we need to justify this approach reveals deeper issues in how we think about accessibility evaluation. When we prioritize traditional user testing over obvious physical measurements, we're not just wasting resources, but we're perpetuating a system that requires marginalized groups to repeatedly demonstrate their exclusion from public spaces and services. Furthermore, mandating user testing without questioning its necessity betrays the very ethos of human factors — we're validating flawed designs through procedural theater rather than addressing fundamental issues.

In the end, this isn't just about kiosks. It's about challenging a research paradigm that values data over observation, compliance over usability, and proof over empathy. Maybe it's time to admit that some things don't need a peer-reviewed study to prove they're problematic - sometimes a measuring tape and basic human empathy are enough.